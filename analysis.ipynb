{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import string\n",
    "\n",
    "try:\n",
    "    import dashscope\n",
    "except ImportError:\n",
    "    dashscope = None\n",
    "\n",
    "def llm(input_text, model='gpt4', stop=[\"\\n\"]):\n",
    "    if model == \"gpt4\":\n",
    "        url = \"http://47.88.8.18:8088/api/ask\"\n",
    "        HTTP_LLM_API_KEY='eyJ0eXAiOiJqd3QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6IjM5NDc3MyIsInBhc3N3b3JkIjoiMzk0NzczMTIzIiwiZXhwIjoyMDIxNjE4MzE3fQ.oQx2Rh-GJ_C29AfHTHE4x_2kVyy7NamwQRKRA4GPA94'\n",
    "        headers = {\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                    \"Authorization\": \"Bearer \" + HTTP_LLM_API_KEY\n",
    "                    }\n",
    "        data = {\n",
    "                \"model\": 'gpt-4',\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_text}\n",
    "                ],\n",
    "                \"n\": 1,\n",
    "                \"temperature\": 0.0\n",
    "                }\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "        response = response.json()\n",
    "        new_response = response['data']['response']\n",
    "    elif model == 'qwen':\n",
    "        api_key = 'sk-94d038e92230451a87ac37ac34dd6a8a'\n",
    "        response = dashscope.Generation.call(\n",
    "            model='qwen-max',\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_text}\n",
    "                ],\n",
    "            result_format=\"message\",  # set the result to be \"message\" format.\n",
    "        )\n",
    "        new_response = response.output\n",
    "    elif model == 'qwen2-57':\n",
    "        api_key = 'sk-94d038e92230451a87ac37ac34dd6a8a'\n",
    "        response = dashscope.Generation.call(\n",
    "            model='qwen2-57b-a14b-instruct',\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_text}\n",
    "                ],\n",
    "            result_format=\"message\",  # set the result to be \"message\" format.\n",
    "        )\n",
    "        new_response = response.output\n",
    "    return new_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def llm_equal(question, gt, pred):\n",
    "    prompt = f\"I have a ground truth answer and a suspect answer for a question. I need to determine if the suspect answer is correct by comparing it to the ground truth answer. Please compare the two answers and let me know if the suspect answer is correct. Please also provide the reason behind your comparison.\\nQuestion:{question}\\nGround Truth Answer: {gt}\\nSuspect Answer: {pred}\\nYou need respond in the following strcure.\\n\\nCorrect:[True or False]\\nReason:\"\n",
    "    return llm(prompt.format(question=question,gt=gt, pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def normalize_answer(s):\n",
    "  def remove_articles(text):\n",
    "    return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "  \n",
    "  def white_space_fix(text):\n",
    "      return \" \".join(text.split())\n",
    "\n",
    "  def remove_punc(text):\n",
    "      exclude = set(string.punctuation)\n",
    "      return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "  def lower(text):\n",
    "      return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "  normalized_prediction = normalize_answer(prediction)\n",
    "  normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "  ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "  if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "    return ZERO_METRIC\n",
    "  if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "    return ZERO_METRIC\n",
    "  \n",
    "  prediction_tokens = normalized_prediction.split()\n",
    "  ground_truth_tokens = normalized_ground_truth.split()\n",
    "  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "  num_same = sum(common.values())\n",
    "  if num_same == 0:\n",
    "    return ZERO_METRIC\n",
    "  precision = 1.0 * num_same / len(prediction_tokens)\n",
    "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/llama3-70_hotpotqa_clean_analysisFalse_fewshotTrue.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m json_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/llama3-70_hotpotqa_clean_analysisFalse_fewshotTrue.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/anaconda3/envs/cok/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/llama3-70_hotpotqa_clean_analysisFalse_fewshotTrue.json'"
     ]
    }
   ],
   "source": [
    "json_file_path = 'output/llama3-70_hotpotqa_clean_analysisFalse_fewshotTrue.json'\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em:0.15217391304347827\n",
      "f1:0.3036231884057971\n"
     ]
    }
   ],
   "source": [
    "ems = [item['answer']==item['gt_answer'] for item in data]\n",
    "f1s = [item['f1'] for item in data]\n",
    "print(f\"em:{sum(ems)/len(ems)}\")\n",
    "print(f\"f1:{sum(f1s)/len(f1s)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def clean_output_up_to_last_observation(output_string): #for Fever dataset only\n",
    "    # Split the text by \"\\n\"\n",
    "    lines = output_string.split(\"\\n\")\n",
    "    \n",
    "    # Initialize the index for the last \"Observation 1:\"\n",
    "    last_observation_index = -1\n",
    "    \n",
    "    # Iterate through the lines to find the last \"Observation 1:\"\n",
    "    for index, line in enumerate(lines):\n",
    "        if line.startswith(\"Observation 1:\"):\n",
    "            last_observation_index = index\n",
    "    \n",
    "    # Extract the relevant part of the text up to the last \"Observation 1:\"\n",
    "    if last_observation_index != -1:\n",
    "        cleaned_output = \"\\n\".join(lines[:last_observation_index])\n",
    "    else:\n",
    "        cleaned_output = output_string  # No \"Observation 1:\" found, return the original string\n",
    "    \n",
    "    return cleaned_output\n",
    "\n",
    "def extract_final_result(cleaned_output):\n",
    "    # Split the text by \"\\n\"\n",
    "    lines = cleaned_output.split(\"\\n\")\n",
    "\n",
    "    # Initialize the final result\n",
    "    final_result = None\n",
    "\n",
    "    # Iterate through the lines to find the last Finish[xxx]\n",
    "    for line in lines:\n",
    "        if \"Finish[\" in line:\n",
    "            # Extract the part inside Finish[xxx]\n",
    "            start_index = line.find(\"Finish[\") + len(\"Finish[\")\n",
    "            end_index = line.find(\"]\", start_index)\n",
    "            if start_index != -1 and end_index != -1:\n",
    "                final_result = line[start_index:end_index]\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    item['answer']=extract_final_result(clean_output_up_to_last_observation(item['traj']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    item['f1']=f1_score(item['answer'], item['gt_answer'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ems = [item['answer']==item['gt_answer'] for item in data]\n",
    "sum(ems)/len(ems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s = [item['f1'] for item in data]\n",
    "sum(f1s)/len(f1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions=[]\n",
    "# with open(\"data/paper_dev.jsonl\") as f:\n",
    "#     for line in f:\n",
    "#         questions.append(json.loads(line.strip()))\n",
    "\n",
    "\n",
    "# llm_eval = []\n",
    "# for item in data:\n",
    "#     llm_eval.append(llm_equal(question=questions[item['question_idx']]['claim'], gt=item['gt_answer'], pred=item['answer']))\n",
    "   \n",
    "    \n",
    "with open(\"data/hotpot_dev_v1_simplified.json\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "llm_eval = []\n",
    "for item in data:\n",
    "    llm_eval.append(llm_equal(question=questions[item['question_idx']]['question'], gt=item['gt_answer'], pred=item['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(text):\n",
    "    # Initialize the dictionary\n",
    "    result_dict = {\"correct\": None, \"reason\": None}\n",
    "    \n",
    "    # Split the output text into lines\n",
    "    if '\\n' not in text:\n",
    "        result_dict['correct']=False\n",
    "        result_dict['reason']=None\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Extract the \"Correct\" and \"Reason\" parts\n",
    "    for line in lines:\n",
    "        if line.startswith('Correct:'):\n",
    "            correct_value = line[len('Correct: '):].strip()\n",
    "            # Convert 'True' and 'False' to boolean values\n",
    "            if correct_value == 'True':\n",
    "                result_dict[\"correct\"] = True\n",
    "            elif correct_value == 'False':\n",
    "                result_dict[\"correct\"] = False\n",
    "        elif line.startswith('Reason:'):\n",
    "            result_dict[\"reason\"] = line[len('Reason: '):].strip()\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_evals=[]\n",
    "for item in llm_eval:\n",
    "    llm_evals.append(parse_output(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43478260869565216"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trues = [item['correct'] for item in llm_evals]\n",
    "sum(trues)/len(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
